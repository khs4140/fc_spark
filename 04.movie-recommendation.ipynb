{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/12 18:10:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>movie-recommendation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10ad084c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "MAX_MEMORY = \"5g\" # config(\"spark.executor.memory\", MAX_memory), config(\"spark.driver.meemory\", MAX_MEMORY)\n",
    "\n",
    "spark = SparkSession.builder.master('local[8]').appName('movie-recommendation').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|    296|   5.0|1147880044|\n",
      "|     1|    306|   3.5|1147868817|\n",
      "|     1|    307|   5.0|1147868828|\n",
      "|     1|    665|   5.0|1147878820|\n",
      "|     1|    899|   3.5|1147868510|\n",
      "|     1|   1088|   4.0|1147868495|\n",
      "|     1|   1175|   3.5|1147868826|\n",
      "|     1|   1217|   3.5|1147878326|\n",
      "|     1|   1237|   5.0|1147868839|\n",
      "|     1|   1250|   4.0|1147868414|\n",
      "|     1|   1260|   3.5|1147877857|\n",
      "|     1|   1653|   4.0|1147868097|\n",
      "|     1|   2011|   2.5|1147868079|\n",
      "|     1|   2012|   2.5|1147868068|\n",
      "|     1|   2068|   2.5|1147869044|\n",
      "|     1|   2161|   3.5|1147868609|\n",
      "|     1|   2351|   4.5|1147877957|\n",
      "|     1|   2573|   4.0|1147878923|\n",
      "|     1|   2632|   5.0|1147878248|\n",
      "|     1|   2692|   5.0|1147869100|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rating_file = '/Users/lifelog/_PML/fc_spark/data/ml-25m/ratings.csv'\n",
    "rating_df = spark.read.csv(f\"file:///{rating_file}\", inferSchema=True, header=True)\n",
    "rating_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = rating_df.select(['userId', 'movieId', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rating_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            rating|\n",
      "+-------+------------------+\n",
      "|  count|          25000095|\n",
      "|   mean| 3.533854451353085|\n",
      "| stddev|1.0607439611423508|\n",
      "|    min|               0.5|\n",
      "|    max|               5.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rating_df.select(['rating']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = rating_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(maxIter=10,\n",
    "          regParam = 0.1,\n",
    "          userCol='userId',\n",
    "          itemCol='movieId',\n",
    "          ratingCol='rating',\n",
    "          coldStartStrategy='drop' # nan or drop\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/12 18:11:41 WARN BlockManager: Block rdd_39_1 could not be removed as it was not found on disk or in memory\n",
      "23/11/12 18:11:41 WARN BlockManager: Block rdd_39_5 could not be removed as it was not found on disk or in memory\n",
      "23/11/12 18:11:41 WARN BlockManager: Block rdd_40_5 could not be removed as it was not found on disk or in memory\n",
      "23/11/12 18:11:41 ERROR Executor: Exception in task 5.0 in stage 9.0 (TID 41)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3793)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4043/0x000000080170c040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4021/0x0000000801712840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3969/0x00000008016fdc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$3977/0x00000008016f6040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "23/11/12 18:11:41 WARN BlockManager: Block rdd_40_1 could not be removed as it was not found on disk or in memory\n",
      "23/11/12 18:11:41 ERROR Executor: Exception in task 1.0 in stage 9.0 (TID 37)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3793)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4043/0x000000080170c040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4021/0x0000000801712840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3969/0x00000008016fdc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$3977/0x00000008016f6040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "23/11/12 18:11:42 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 9.0 (TID 37),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3793)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4043/0x000000080170c040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4021/0x0000000801712840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3969/0x00000008016fdc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$3977/0x00000008016f6040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "23/11/12 18:11:42 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 9.0 (TID 41),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3793)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4043/0x000000080170c040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4021/0x0000000801712840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3969/0x00000008016fdc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$3977/0x00000008016f6040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "23/11/12 18:11:42 WARN TaskSetManager: Lost task 5.0 in stage 9.0 (TID 41) (localhost executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3793)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4043/0x000000080170c040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4021/0x0000000801712840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3969/0x00000008016fdc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$3977/0x00000008016f6040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\n",
      "23/11/12 18:11:42 ERROR TaskSetManager: Task 5 in stage 9.0 failed 1 times; aborting job\n",
      "23/11/12 18:11:42 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@4066f1e0 rejected from java.util.concurrent.ThreadPoolExecutor@424ef138[Shutting down, pool size = 7, active threads = 7, queued tasks = 0, completed tasks = 37]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/11/12 18:11:42 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 9.0 failed 1 times, most recent failure: Lost task 5.0 in stage 9.0 (TID 41) (localhost executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3793)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4043/0x000000080170c040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4021/0x0000000801712840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3969/0x00000008016fdc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$3977/0x00000008016f6040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1293)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:988)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:737)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:714)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:616)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3793)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$4043/0x000000080170c040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$4021/0x0000000801712840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3969/0x00000008016fdc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$3977/0x00000008016f6040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2330/0x0000000801152040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/pv/7rcpvrpd6t9340mk69b10r240000gn/T/ipykernel_1351/1003521471.py\", line 1, in <module>\n",
      "    als_model = als.fit(train_df)\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m/Users/lifelog/_PML/fc_spark/04.movie-recommendation.ipynb ì…€ 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lifelog/_PML/fc_spark/04.movie-recommendation.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m als_model \u001b[39m=\u001b[39m als\u001b[39m.\u001b[39;49mfit(train_df)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2113\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2110\u001b[0m     traceback\u001b[39m.\u001b[39mprint_exc()\n\u001b[1;32m   2111\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2113\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_showtraceback(etype, value, stb)\n\u001b[1;32m   2114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_pdb:\n\u001b[1;32m   2115\u001b[0m     \u001b[39m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebugger(force\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/ipykernel/zmqshell.py:541\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    535\u001b[0m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    536\u001b[0m sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    538\u001b[0m exc_content \u001b[39m=\u001b[39m {\n\u001b[1;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtraceback\u001b[39m\u001b[39m\"\u001b[39m: stb,\n\u001b[1;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mename\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(etype\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m),\n\u001b[0;32m--> 541\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevalue\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39;49m(evalue),\n\u001b[1;32m    542\u001b[0m }\n\u001b[1;32m    544\u001b[0m dh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayhook\n\u001b[1;32m    545\u001b[0m \u001b[39m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[39m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_exception\u001b[39m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[39m=\u001b[39m gateway_client\u001b[39m.\u001b[39;49msend_command(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexception_cmd)\n\u001b[1;32m    472\u001b[0m     return_value \u001b[39m=\u001b[39m get_return_value(answer, gateway_client, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[39m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[39m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[39m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "als_model = als.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = als_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 138:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "|     1|    306|   3.5| 4.0662317|\n",
      "|     1|   1260|   3.5| 4.0207963|\n",
      "|     1|   2068|   2.5| 3.8897588|\n",
      "|     1|   3949|   5.0|  3.921198|\n",
      "|     1|   4325|   5.0|   3.98439|\n",
      "|     1|   5952|   4.0| 3.6899786|\n",
      "|     1|   6711|   5.0|  3.989763|\n",
      "|     1|   7327|   3.5| 4.0020003|\n",
      "|     1|   7361|   5.0|  4.146472|\n",
      "|     1|   7937|   3.0| 3.8399374|\n",
      "|     1|   8729|   3.5| 3.0515068|\n",
      "|     1|   8786|   4.0| 3.7612777|\n",
      "|     1|   8973|   4.0| 3.8708837|\n",
      "|     1|  27266|   4.5| 3.7075858|\n",
      "|    12|     16|   5.0| 3.5409687|\n",
      "|    12|     20|   1.0|  2.143339|\n",
      "|    12|    101|   4.0|  3.627047|\n",
      "|    12|    185|   2.0| 2.3618746|\n",
      "|    12|    186|   2.5| 2.2741473|\n",
      "|    12|    222|   3.0| 3.2050283|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 214:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|            rating|        prediction|\n",
      "+-------+------------------+------------------+\n",
      "|  count|           4997655|           4997655|\n",
      "|   mean|3.5341622220821565|3.4138912451126617|\n",
      "| stddev|1.0609444014040805|0.6459231419320535|\n",
      "|    min|               0.5|          -1.25035|\n",
      "|    max|               5.0|         6.6312466|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select(['rating', 'prediction']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction',\n",
    "                    labelCol='rating',\n",
    "                    metricName='rmse',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 317:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8069730161822684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (2 + 4) / 100]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/lifelog/miniconda3/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lifelog/_PML/fc_spark/04.movie-recommendation.ipynb ì…€ 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lifelog/_PML/fc_spark/04.movie-recommendation.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m als_model\u001b[39m.\u001b[39;49mrecommendForAllUsers(\u001b[39m3\u001b[39;49m)\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    960\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# als_model.recommendForAllUsers(3).show()\n",
    "als_model.recommendForUserSubset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_model.recommendForAllItems(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 341:>              (2 + 4) / 100][Stage 342:>                (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "user_list = [65, 78, 81]\n",
    "spark.createDataFrame(user_list, IntegerType()).toDF('userId').toDF('userId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
